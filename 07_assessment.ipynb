{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on going through today's course! Hopefully, you've learned some valuable skills along the way and had fun doing it. Now it's time to put those skills to the test. In this assessment, you will train a new model that is able to recognize fresh and rotten fruit. You will need to get the model to a validation accuracy of `92%` in order to pass the assessment, though we challenge you to do even better if you can. You will have the use the skills that you learned in the previous exercises. Specifically, we suggest using some combination of transfer learning, data augmentation, and fine tuning. Once you have trained the model to be at least 92% accurate on the validation dataset, save your model, and then assess its accuracy. Let's get started! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.io as tv_io\n",
    "\n",
    "import glob\n",
    "from PIL import Image\n",
    "\n",
    "import utils\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 The Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a model to recognize fresh and rotten fruits. The dataset comes from [Kaggle](https://www.kaggle.com/sriramr/fruits-fresh-and-rotten-for-classification), a great place to go if you're interested in starting a project after this class. The dataset structure is in the `data/fruits` folder. There are 6 categories of fruits: fresh apples, fresh oranges, fresh bananas, rotten apples, rotten oranges, and rotten bananas. This will mean that your model will require an output layer of 6 neurons to do the categorization successfully. You'll also need to compile the model with `categorical_crossentropy`, as we have more than two categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fruits.png\" style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Load ImageNet Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to start with a model pretrained on ImageNet. Load the model with the correct weights. Because these pictures are in color, there will be three channels for red, green, and blue. We've filled in the input shape for you. If you need a reference for setting up the pretrained model, please take a look at [notebook 05b](05b_presidential_doggy_door.ipynb) where we implemented transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "weights = VGG16_Weights.DEFAULT\n",
    "vgg_model = vgg16(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Freeze Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we suggest freezing the base model, as done in [notebook 05b](05b_presidential_doggy_door.ipynb). This is done so that all the learning from the ImageNet dataset does not get destroyed in the initial training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze base model\n",
    "vgg_model.requires_grad_(False)\n",
    "next(iter(vgg_model.parameters())).requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Add Layers to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to add layers to the pretrained model. [Notebook 05b](05b_presidential_doggy_door.ipynb) can be used as a guide. Pay close attention to the last dense layer and make sure it has the correct number of neurons to classify the different types of fruit.\n",
    "\n",
    "The later layers of a model become more specific to the data the model trained on. Since we want the more general learnings from VGG, we can select parts of it, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model.classifier[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've taken what we've wanted from VGG16, then we can add our own modifications. No matter what additional modules we add, we still need to end with one value for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 6\n",
    "\n",
    "my_model = nn.Sequential(\n",
    "    vgg_model.features,\n",
    "    vgg_model.avgpool,\n",
    "    nn.Flatten(),\n",
    "    vgg_model.classifier[0:3],\n",
    "    nn.Linear(4096, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, N_CLASSES)\n",
    ")\n",
    "# my_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to compile the model with loss and metrics options. We have 6 classes, so which loss function should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() # BCELoss\n",
    "optimizer = Adam(my_model.parameters())\n",
    "my_model = torch.compile(my_model.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Data Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess our input images, we will use the transforms included with the VGG16 weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trans = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to randomly augment the data to improve the dataset. Feel free to look at [notebook 04a](04a_asl_augmentation.ipynb) and [notebook 05b](05b_presidential_doggy_door.ipynb) for augmentation examples. There is also documentation for the [TorchVision Transforms class](https://pytorch.org/vision/stable/transforms.html).\n",
    "\n",
    "**Hint**: Remember not to make the data augmentation too extreme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "IMG_WIDTH, IMG_HEIGHT = (224, 224)\n",
    "\n",
    "\"\"\"random_trans = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=10),\n",
    "    # transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(.7, 1), ratio=(1, 1)),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\"\"\"\n",
    "\n",
    "random_trans = transforms.Compose([\n",
    "    transforms.RandomResizedCrop((IMG_WIDTH, IMG_HEIGHT), scale=(0.8, 1.0)),  # Adjust scale range\n",
    "    transforms.RandomHorizontalFlip(),  # Randomly flip the image horizontally\n",
    "    transforms.RandomRotation(30),  # Rotate by smaller degrees\n",
    "    transforms.ColorJitter(brightness=0.9, contrast=0.2, saturation=0.2, hue=0.1),  # Fine-tune ColorJitter\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=5),  # Adjust RandomAffine parameters\n",
    "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Add slight blur\n",
    "    transforms.ToTensor(),  # Convert the image to a tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the image\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to load the train and validation datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LABELS = [\"freshapples\", \"freshbanana\", \"freshoranges\", \"rottenapples\", \"rottenbanana\", \"rottenoranges\"] \n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for l_idx, label in enumerate(DATA_LABELS):\n",
    "            data_paths = glob.glob(data_dir + label + '/*.png', recursive=True)\n",
    "            for path in data_paths:\n",
    "                img = tv_io.read_image(path, tv_io.ImageReadMode.RGB)\n",
    "                self.imgs.append(pre_trans(img).to(device))\n",
    "                self.labels.append(torch.tensor(l_idx).to(device))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.imgs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the batch size `n` and set `shuffle` either to `True` or `False` depending on if we are `train`ing or `valid`ating. For a reference, check out [notebook 05b](05b_presidential_doggy_door.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 32\n",
    "\n",
    "train_path = \"data/fruits/train/\"\n",
    "train_data = MyDataset(train_path)\n",
    "train_loader = DataLoader(train_data, batch_size=n, shuffle=True)\n",
    "train_N = len(train_loader.dataset)\n",
    "\n",
    "valid_path = \"data/fruits/valid/\"\n",
    "valid_data = MyDataset(valid_path)\n",
    "valid_loader = DataLoader(valid_data, batch_size=n, shuffle=False)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to train the model! We've moved the `train` and `validate` functions to our [utils.py](./utils.py) file. Before running the below, make sure all your variables are correctly defined.\n",
    "\n",
    "It may help to rerun this cell or change the number of `epochs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 35.7339 Accuracy: 0.6438\n",
      "Valid - Loss: 9.1091 Accuracy: 0.7872\n",
      "Epoch: 1\n",
      "Train - Loss: 23.7256 Accuracy: 0.7606\n",
      "Valid - Loss: 5.1079 Accuracy: 0.8723\n",
      "Epoch: 2\n",
      "Train - Loss: 22.8164 Accuracy: 0.7707\n",
      "Valid - Loss: 3.1132 Accuracy: 0.9210\n",
      "Epoch: 3\n",
      "Train - Loss: 19.0165 Accuracy: 0.8147\n",
      "Valid - Loss: 3.4905 Accuracy: 0.8967\n",
      "Epoch: 4\n",
      "Train - Loss: 20.5055 Accuracy: 0.8020\n",
      "Valid - Loss: 4.5011 Accuracy: 0.8632\n",
      "Epoch: 5\n",
      "Train - Loss: 17.6149 Accuracy: 0.8164\n",
      "Valid - Loss: 4.1142 Accuracy: 0.8936\n",
      "Epoch: 6\n",
      "Train - Loss: 17.2605 Accuracy: 0.8316\n",
      "Valid - Loss: 4.9464 Accuracy: 0.8723\n",
      "Epoch: 7\n",
      "Train - Loss: 17.2594 Accuracy: 0.8240\n",
      "Valid - Loss: 6.0733 Accuracy: 0.8511\n",
      "Epoch: 8\n",
      "Train - Loss: 17.9363 Accuracy: 0.8232\n",
      "Valid - Loss: 6.5787 Accuracy: 0.8754\n",
      "Epoch: 9\n",
      "Train - Loss: 18.0630 Accuracy: 0.8096\n",
      "Valid - Loss: 2.9412 Accuracy: 0.9119\n",
      "Epoch: 10\n",
      "Train - Loss: 14.1845 Accuracy: 0.8477\n",
      "Valid - Loss: 4.6436 Accuracy: 0.8997\n",
      "Epoch: 11\n",
      "Train - Loss: 16.8470 Accuracy: 0.8308\n",
      "Valid - Loss: 4.0729 Accuracy: 0.9179\n",
      "Epoch: 12\n",
      "Train - Loss: 14.6065 Accuracy: 0.8604\n",
      "Valid - Loss: 5.8744 Accuracy: 0.8815\n",
      "Epoch: 13\n",
      "Train - Loss: 15.4350 Accuracy: 0.8418\n",
      "Valid - Loss: 3.8186 Accuracy: 0.9179\n",
      "Epoch: 14\n",
      "Train - Loss: 15.2528 Accuracy: 0.8519\n",
      "Valid - Loss: 3.3049 Accuracy: 0.9331\n",
      "Epoch: 15\n",
      "Train - Loss: 15.4485 Accuracy: 0.8376\n",
      "Valid - Loss: 2.1486 Accuracy: 0.9453\n",
      "Epoch: 16\n",
      "Train - Loss: 13.8051 Accuracy: 0.8646\n",
      "Valid - Loss: 4.1493 Accuracy: 0.9179\n",
      "Epoch: 17\n",
      "Train - Loss: 14.3846 Accuracy: 0.8435\n",
      "Valid - Loss: 2.5885 Accuracy: 0.9240\n",
      "Epoch: 18\n",
      "Train - Loss: 13.4380 Accuracy: 0.8629\n",
      "Valid - Loss: 4.0800 Accuracy: 0.9088\n",
      "Epoch: 19\n",
      "Train - Loss: 14.1809 Accuracy: 0.8621\n",
      "Valid - Loss: 3.3770 Accuracy: 0.9149\n",
      "Epoch: 20\n",
      "Train - Loss: 14.8260 Accuracy: 0.8494\n",
      "Valid - Loss: 3.8761 Accuracy: 0.8997\n",
      "Epoch: 21\n",
      "Train - Loss: 13.8121 Accuracy: 0.8731\n",
      "Valid - Loss: 2.7905 Accuracy: 0.9301\n",
      "Epoch: 22\n",
      "Train - Loss: 13.5448 Accuracy: 0.8655\n",
      "Valid - Loss: 4.6313 Accuracy: 0.9027\n",
      "Epoch: 23\n",
      "Train - Loss: 14.9063 Accuracy: 0.8553\n",
      "Valid - Loss: 3.3184 Accuracy: 0.9179\n",
      "Epoch: 24\n",
      "Train - Loss: 11.4621 Accuracy: 0.8748\n",
      "Valid - Loss: 2.6437 Accuracy: 0.9149\n",
      "Epoch: 25\n",
      "Train - Loss: 12.2231 Accuracy: 0.8706\n",
      "Valid - Loss: 4.4708 Accuracy: 0.8997\n",
      "Epoch: 26\n",
      "Train - Loss: 13.4295 Accuracy: 0.8680\n",
      "Valid - Loss: 4.0114 Accuracy: 0.9179\n",
      "Epoch: 27\n",
      "Train - Loss: 11.6407 Accuracy: 0.8832\n",
      "Valid - Loss: 3.8034 Accuracy: 0.9119\n",
      "Epoch: 28\n",
      "Train - Loss: 13.3189 Accuracy: 0.8689\n",
      "Valid - Loss: 4.3355 Accuracy: 0.9058\n",
      "Epoch: 29\n",
      "Train - Loss: 10.2091 Accuracy: 0.8968\n",
      "Valid - Loss: 3.8037 Accuracy: 0.9210\n",
      "Epoch: 30\n",
      "Train - Loss: 12.3474 Accuracy: 0.8773\n",
      "Valid - Loss: 7.5913 Accuracy: 0.8541\n",
      "Epoch: 31\n",
      "Train - Loss: 10.9931 Accuracy: 0.8849\n",
      "Valid - Loss: 4.5653 Accuracy: 0.8997\n",
      "Epoch: 32\n",
      "Train - Loss: 15.2775 Accuracy: 0.8443\n",
      "Valid - Loss: 3.6258 Accuracy: 0.9088\n",
      "Epoch: 33\n",
      "Train - Loss: 13.3934 Accuracy: 0.8638\n",
      "Valid - Loss: 3.6425 Accuracy: 0.9240\n",
      "Epoch: 34\n",
      "Train - Loss: 13.7798 Accuracy: 0.8621\n",
      "Valid - Loss: 7.3180 Accuracy: 0.8632\n",
      "Epoch: 35\n",
      "Train - Loss: 11.1208 Accuracy: 0.8909\n",
      "Valid - Loss: 5.5410 Accuracy: 0.8845\n",
      "Epoch: 36\n",
      "Train - Loss: 12.1690 Accuracy: 0.8790\n",
      "Valid - Loss: 2.8868 Accuracy: 0.9210\n",
      "Epoch: 37\n",
      "Train - Loss: 12.8689 Accuracy: 0.8663\n",
      "Valid - Loss: 4.0440 Accuracy: 0.8936\n",
      "Epoch: 38\n",
      "Train - Loss: 12.4778 Accuracy: 0.8655\n",
      "Valid - Loss: 4.6017 Accuracy: 0.8967\n",
      "Epoch: 39\n",
      "Train - Loss: 12.1005 Accuracy: 0.8807\n",
      "Valid - Loss: 3.7495 Accuracy: 0.9027\n",
      "Epoch: 40\n",
      "Train - Loss: 12.4149 Accuracy: 0.8655\n",
      "Valid - Loss: 3.0585 Accuracy: 0.9240\n",
      "Epoch: 41\n",
      "Train - Loss: 11.0613 Accuracy: 0.8917\n",
      "Valid - Loss: 3.0837 Accuracy: 0.9179\n",
      "Epoch: 42\n",
      "Train - Loss: 12.1419 Accuracy: 0.8849\n",
      "Valid - Loss: 3.4627 Accuracy: 0.9058\n",
      "Epoch: 43\n",
      "Train - Loss: 12.3053 Accuracy: 0.8739\n",
      "Valid - Loss: 3.9926 Accuracy: 0.9119\n",
      "Epoch: 44\n",
      "Train - Loss: 11.4169 Accuracy: 0.8875\n",
      "Valid - Loss: 3.7743 Accuracy: 0.9088\n",
      "Epoch: 45\n",
      "Train - Loss: 11.7813 Accuracy: 0.8756\n",
      "Valid - Loss: 3.8437 Accuracy: 0.9058\n",
      "Epoch: 46\n",
      "Train - Loss: 13.1168 Accuracy: 0.8739\n",
      "Valid - Loss: 3.8324 Accuracy: 0.9149\n",
      "Epoch: 47\n",
      "Train - Loss: 12.6125 Accuracy: 0.8773\n",
      "Valid - Loss: 4.5922 Accuracy: 0.9119\n",
      "Epoch: 48\n",
      "Train - Loss: 11.8893 Accuracy: 0.8875\n",
      "Valid - Loss: 4.7313 Accuracy: 0.8936\n",
      "Epoch: 49\n",
      "Train - Loss: 10.9287 Accuracy: 0.8875\n",
      "Valid - Loss: 3.6789 Accuracy: 0.9119\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Unfreeze Model for Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have reached 92% validation accuracy already, this next step is optional. If not, we suggest fine tuning the model with a very low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the base model\n",
    "vgg_model.requires_grad_(True)\n",
    "optimizer = Adam(my_model.parameters(), lr=.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 11.1674 Accuracy: 0.8866\n",
      "Valid - Loss: 4.3784 Accuracy: 0.9088\n",
      "Epoch: 1\n",
      "Train - Loss: 9.8212 Accuracy: 0.8934\n",
      "Valid - Loss: 4.2433 Accuracy: 0.9179\n",
      "Epoch: 2\n",
      "Train - Loss: 9.5077 Accuracy: 0.8985\n",
      "Valid - Loss: 4.0491 Accuracy: 0.9210\n",
      "Epoch: 3\n",
      "Train - Loss: 8.7090 Accuracy: 0.9086\n",
      "Valid - Loss: 4.2282 Accuracy: 0.9179\n",
      "Epoch: 4\n",
      "Train - Loss: 9.3925 Accuracy: 0.9103\n",
      "Valid - Loss: 4.1897 Accuracy: 0.9119\n",
      "Epoch: 5\n",
      "Train - Loss: 9.0549 Accuracy: 0.9069\n",
      "Valid - Loss: 4.2781 Accuracy: 0.9119\n",
      "Epoch: 6\n",
      "Train - Loss: 9.0177 Accuracy: 0.9027\n",
      "Valid - Loss: 4.4245 Accuracy: 0.9149\n",
      "Epoch: 7\n",
      "Train - Loss: 9.8786 Accuracy: 0.9002\n",
      "Valid - Loss: 4.0409 Accuracy: 0.9210\n",
      "Epoch: 8\n",
      "Train - Loss: 8.4409 Accuracy: 0.9239\n",
      "Valid - Loss: 4.4475 Accuracy: 0.9119\n",
      "Epoch: 9\n",
      "Train - Loss: 9.2533 Accuracy: 0.9036\n",
      "Valid - Loss: 3.9119 Accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    utils.train(my_model, train_loader, train_N, random_trans, optimizer, loss_function)\n",
    "    utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.10 Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you now have a model that has a validation accuracy of 92% or higher. If not, you may want to go back and either run more epochs of training, or adjust your data augmentation. \n",
    "\n",
    "Once you are satisfied with the validation accuracy, evaluate the model by executing the following cell. The evaluate function will return a tuple, where the first value is your loss, and the second value is your accuracy. To pass, the model will need have an accuracy value of `92% or higher`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid - Loss: 3.9119 Accuracy: 0.9240\n"
     ]
    }
   ],
   "source": [
    "utils.validate(my_model, valid_loader, valid_N, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.11 Run the Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess your model run the following two cells.\n",
    "\n",
    "**NOTE:** `run_assessment` assumes your model is named `my_model`. If for any reason you have modified these variable names, please update the names of the arguments passed to `run_assessment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from run_assessment import run_assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model to obtain average accuracy...\n",
      "\n",
      "Accuracy: 0.9240\n",
      "\n",
      "Accuracy required to pass the assessment is 0.92 or greater.\n",
      "Your average accuracy is 0.9240.\n",
      "\n",
      "Congratulations! You passed the assessment!\n",
      "See instructions below to generate a certificate.\n"
     ]
    }
   ],
   "source": [
    "run_assessment(my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.12 Generate a Certificate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you passed the assessment, please return to the course page (shown below) and click the \"ASSESS TASK\" button, which will generate your certificate for the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/assess_task.png\" style=\"width: 800px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><a href=\"https://www.nvidia.com/dli\"> <img src=\"images/DLI_Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
